{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdcf65fc",
   "metadata": {},
   "source": [
    "## Installation of libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e00fca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install xgboost -U\n",
    "#!pip install librosa -U\n",
    "''''librosa is the package for music and audio analysis'''\n",
    "#!pip install hyperopt\n",
    "'''hyperopt is designed to accommodate Bayesian optimization algorithms based on Gaussian processes and regression tress.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c40c0b7",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be98353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "# used to display progress bars which are used in this program\n",
    "import pickle\n",
    "'''\n",
    "pickle is the  process of converting a Python object into a byte stream to store it in a file/database, \n",
    "maintain program state across sessions, or transport data over the network.\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import librosa\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFECV,mutual_info_regression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "'''\n",
    "eXtreme Gradient Boosting (XGBoost) is a scalable and improved version of the gradient boosting algorithm \n",
    "designed for efficacy, computational speed and model performance. \n",
    "It is an open-source library and a part of the Distributed Machine Learning Community. \n",
    "XGBoost is a perfect blend of software and hardware capabilities designed to enhance existing boosting techniques with accuracy \n",
    "in the shortest amount of time.\n",
    "'''\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "#For hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d17cd8",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d4860",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# we are only considering 30 seconds files \n",
    "because it is still difficult to determine the genre of music based off of 3 seconds.\n",
    "'''\n",
    "df = pd.read_csv('Data/features_30_sec.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623a44cd",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086c1077",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts()\n",
    "# to check how many genres are present and how many songs each genre has\n",
    "\n",
    "df.info()\n",
    "df.describe()\n",
    "'''\n",
    "Only 1000 songs are in the dataset with no null values\n",
    "length is not a relevant variable which needs to be dropped in model training phase\n",
    "There are some other music features that this dataset did not extract, \n",
    "for example, spectral contrast, spectral flatness, tonal centroid features etc. \n",
    "Given it is just a small dataset, we can extract them and put it in the dataframe for further analysis\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dc9631",
   "metadata": {},
   "source": [
    "## Extracting extra features and data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_path = 'Data/genres_original'\n",
    "\n",
    "# code to extract the 4 additional features from the 1000 music files present in the dataset.\n",
    "def extract_new_features(song_path, num_files = 1000, num_new_features = 8):\n",
    "    data_array = np.empty([num_files, num_new_features])\n",
    "    counter = 0\n",
    "    for root, dirs, files in os.walk(songs_path):\n",
    "        dirs.sort()\n",
    "        for file, i in zip(sorted(files), tqdm(range(num_files))):\n",
    "            i = i + (counter-1)*100\n",
    "            file_path = os.path.join(root, file)\n",
    "        \n",
    "            try:\n",
    "                #extract mean and variance of those 4 features\n",
    "                y, sr = librosa.load(os.fspath(file_path))\n",
    "                chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)\n",
    "                spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "                spectral_flatness = librosa.feature.spectral_flatness(y=y)\n",
    "                tonnetz = librosa.feature.tonnetz(y=y, sr=sr)\n",
    "\n",
    "                data_array[i,0] = np.mean(chroma_cens)\n",
    "                data_array[i,1] = np.var(chroma_cens)\n",
    "                data_array[i,2] = np.mean(spectral_contrast)\n",
    "                data_array[i,3] = np.var(spectral_contrast)\n",
    "                data_array[i,4] = np.mean(spectral_flatness)\n",
    "                data_array[i,5] = np.var(spectral_flatness)\n",
    "                data_array[i,6] = np.mean(tonnetz)\n",
    "                data_array[i,7] = np.var(tonnetz)\n",
    "                \n",
    "            # Set all values to zero for files with problems\n",
    "            except:\n",
    "                print(f'Problem file: {file_path}')\n",
    "                for j in range(num_new_features):\n",
    "                    data_array[i, j] = 0 \n",
    "                \n",
    "        counter += 1\n",
    "                  \n",
    "    return data_array\n",
    "\n",
    "\n",
    "new_features_array = extract_new_features('Data/genres_original')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7c785f",
   "metadata": {},
   "source": [
    "## Adding the above extracted 4 features to the already selected 4 features which are present in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc3068",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['chroma_cens_mean'] = new_features_array[:,0]\n",
    "df['chroma_cens_var'] = new_features_array[:,1]\n",
    "df['spectral_contrast_mean'] = new_features_array[:,2]\n",
    "df['spectral_contrast_var'] = new_features_array[:,3]\n",
    "df['spectral_flatness_mean'] = new_features_array[:,4]\n",
    "df['spectral_flatness_var'] = new_features_array[:,5]\n",
    "df['tonnetz_mean'] = new_features_array[:,6]\n",
    "df['tonnetz_var'] = new_features_array[:,7]\n",
    "'''\n",
    "Since there is only 1 row of missing value, \n",
    "now we'll just fill in the missing values of jazz.0054.wav as the mean of those of the jazz songs, which is at index 554\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259198c1",
   "metadata": {},
   "source": [
    "## Filtering out the jazz genre expect for jazz.0054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e6e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(-8,0,1):\n",
    "    # Filter out the jazz genre except jazz.0054\n",
    "    df.iloc[554,i] = df[ df.label =='jazz'].iloc[np.r_[np.arange(0,54),np.arange(55,100)],i].mean()\n",
    "\n",
    "df.to_csv('new_csv',index=False)\n",
    "# saving the index without the index column which comes by default\n",
    "\n",
    "df = df.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5ae01f",
   "metadata": {},
   "source": [
    "## EDA and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b5bf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "\n",
    "#Create a mask for the heatmap\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "plt.subplots(figsize=(20, 20))\n",
    "sns.heatmap(corr, mask=mask, cmap=\"vlag\")\n",
    "''''\n",
    "Most of the variables do not have a high correlation with other variables. \n",
    "Let's filter out the extremely highly correlated pairs and examine them.\n",
    "'''\n",
    "\n",
    "sol = (corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\n",
    "                  .stack()\n",
    "                  .sort_values(ascending=False))\n",
    "for index, value in sol.items():\n",
    "    if (value > 0.75) or (value < -0.75):\n",
    "        print(index, value)\n",
    "'''\n",
    "using this code we are able to extract the features which are correlated with each other and \n",
    "also by how much from the above map\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f82a45e",
   "metadata": {},
   "source": [
    "## Splitting train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f8425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Given the small number of training data, \n",
    "We set 90% as training data and 10% as testing data. \n",
    "For hyparameter tuning, given the small dataset, we will use the same train dataset to tune For the split of data, \n",
    "We made sure every class has the same number of data to train and test.\n",
    "'''\n",
    "y = df.label\n",
    "X = df\n",
    "\n",
    "#Use `label` to split data evenly and drop `label` column after split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=df.label, random_state=77)\n",
    "X_train.drop('label',axis=1,inplace=True)\n",
    "X_test.drop('label',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f7363d",
   "metadata": {},
   "source": [
    "## Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e532b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
    "X_test_scaled = sc.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test_scaled, index=X_test.index, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959dbaf2",
   "metadata": {},
   "source": [
    "## Initial model fitting and recursive feature elimination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe36896",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we will fit our training data to xgboost classifier first, and \n",
    "then we’ll do RFECV to check which variables can be eliminated\n",
    "RFECV stands for Recursive Feature Elimination and Cross-Validation Selection \n",
    "which does Backward selection in Python: Scikit-Learn.\n",
    "'''\n",
    "estimator = XGBClassifier(eval_metric='merror')\n",
    "rfecv = RFECV(estimator, step=1, cv=5,scoring='accuracy',verbose=1)\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "# if step > 1 then step corresponds to the (integer) number of features to remove at each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9472606",
   "metadata": {},
   "source": [
    "## Checking which features can be further deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b65ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_drop_array = list(np.where(rfecv.support_ == False)[0])\n",
    "X_train.columns[features_drop_array]\n",
    "\n",
    "X_train.drop(X_train.columns[features_drop_array], axis=1, inplace=True)\n",
    "X_test.drop(X_test.columns[features_drop_array], axis=1, inplace=True)\n",
    "\n",
    "# this code determines which columns can be eliminated and drops them, this happens in both train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a91890",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d3b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(n_estimators=1000)\n",
    "model.fit(X_train,y_train,eval_metric='merror')\n",
    "'''\n",
    "eval_metric is the evaluation metric for validation data, \n",
    "a default metric will be assigned according to objective (rmse for regression, and logloss for classification) \n",
    "but here we use rmse -> root mean square error for better yielding results\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9140767",
   "metadata": {},
   "source": [
    "## Getting the accuracy for both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cc6b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "target_names = sorted(set(y))\n",
    "\n",
    "print(f'Training accuracy: {accuracy_score(y_train,y_pred_train)}')\n",
    "print(f'Training:\\n {classification_report(y_train, y_pred_train, labels=target_names)}')\n",
    "print(f'Testing accuracy: {accuracy_score(y_test,y_pred_test)}')\n",
    "print(f'Testing:\\n {classification_report(y_test, y_pred_test, labels=target_names)}')\n",
    "\n",
    "#Confusion matrix of the test data\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize = (16, 9))\n",
    "sns.heatmap(cm,cmap=\"Blues\", annot=True, xticklabels = target_names, yticklabels = target_names )\n",
    "\n",
    "'''\n",
    "this will display training accuracy as 0.99 while test data is 0.78 \n",
    "which indicates that the model has been overfit. \n",
    "Hence, we will add regularization parameters and tune other parameters to reduce this problem.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab81cd81",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "When creating a machine learning model, you'll be presented with design choices as to how to define your model architecture. \n",
    "Often times, we don't immediately know what the optimal model architecture should be for a given model, \n",
    "and thus we'd like to be able to explore a range of possibilities. \n",
    "In true machine learning fashion, we'll ideally ask the machine to perform this exploration and \n",
    "select the optimal model architecture automatically. \n",
    "Parameters which define the model architecture are referred to as hyperparameters and \n",
    "thus this process of searching for the ideal model architecture is referred to as hyperparameter tuning.\n",
    "'''\n",
    "# In the following, we'll use hyperopt library to help tuning the parameters. The parameters may vary for each run\n",
    "\n",
    "space={\n",
    "    'n_estimators': hp.quniform('n_estimators', 0,3000,1),\n",
    "    'reg_lambda' : hp.quniform('reg_lambda', 0,500,1),\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    clf=XGBClassifier(\n",
    "                    n_estimators =int(space['n_estimators']),\n",
    "                    reg_lambda = int(space['reg_lambda']),\n",
    "                    )\n",
    "    \n",
    "    evaluation = [( X_train, y_train), ( X_test, y_test)]\n",
    "    \n",
    "    clf.fit(X_train, y_train,\n",
    "            eval_set=evaluation, eval_metric=\"auc\",\n",
    "            early_stopping_rounds=10,verbose=False)\n",
    "    \n",
    "\n",
    "    pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best_hyperparams = fmin(fn = objective,\n",
    "                        space = space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = 100,\n",
    "                        trials = trials)\n",
    "\n",
    "print(f\"best params: {best_hyperparams}\")\n",
    "\n",
    "# this will output the best n_estimators and reg_lambda values to be used for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2540fa96",
   "metadata": {},
   "source": [
    "## Running the model again after hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b529b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = XGBClassifier(n_estimators=304, reg_lambda=25)\n",
    "model1.fit(X_train,y_train,eval_metric='merror')\n",
    "y_pred_test1 = model1.predict(X_test)\n",
    "print(f\"accuracy: {accuracy_score(y_test,y_pred_test1)}\")\n",
    "print(f'New tuned model:\\n {classification_report(y_test, y_pred_test1, labels=target_names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee512d9",
   "metadata": {},
   "source": [
    "## Save the model and the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4c3222",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(sc, open('sc.pkl','wb'))\n",
    "pickle.dump(model1, open('model.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
